\documentclass{article}
\usepackage{amsmath}
\usepackage{color}
\usepackage{hyperref} %%http://www.johndcook.com/blog/2008/11/24/link-to-web-pages-from-latex-pdf/
\begin{document}

\title{Dirichlet Process Mixture of Bernoulli Vectors}
\maketitle

\section{Model}

\begin{equation}
\begin{matrix}
\alpha & \sim & Gamma(1,1) \\
\beta_d & \sim & Gamma(1,1) \\
z_i & \sim & CRP(\alpha) \\
\theta_{d,c} & \sim & Beta(\beta_d,\beta_d) \\
x_{i,d} & \sim & Binomial(\theta_{d,c}) \\
\end{matrix}
\end{equation}

From VKM's \href{https://docs.google.com/document/d/1FolRwjOY2Uc2GghywjhJmX-IcEiRG1S2rEZdGYmIHm8/edit}{Excercises on Engineering Stochastic Systems}

\begin{equation}
\begin{matrix}
P(\vec Z | \vec X) & = & \frac{P(\vec X | \vec Z)P(\vec Z)}{P(\vec X)} \\
 & \propto & P(\vec X | \vec Z)P(\vec Z)
\end{matrix}
\end{equation}

All models must use $P(\vec X)$ so its just a multiplicative constant and can be dropped when only concerned with inference on $\vec Z$.

\section{Notation}
\begin{equation}
\begin{matrix}
b_c & = & \{i | z_i = c\} \\
N_c &  = & \|b_c\| \\
B & = & \{b_c | c \in \{z_i\}\} \\

S_{c,d} & = & \sum_{i \in b_c} X_{i,d} \\
R_{c,d} & = & N_c - S_{c,d} \\
\end{matrix}
\end{equation}

$b_c$ is the set of indices which belong to the c-th cluster

$N_c$ is the number of vectors assigned to the c-th cluster $c$

$S_{c,d}$ is the number of vectors assigned to the c-th cluster which have their d-th element equal to 1 (\textbf{S}um)

$R_{c,d}$ is the number of vectors assigned to the c-th cluster which have their d-th element equal to 0 (\textbf{R}emainder)

$N$ is the number of vectors

$D$ is the number of elements in each vector

$X$ is a boolean array (the data) : $\{0,1\}^{N \times D}$

\section{Derivation of $P(X|Z)$}

Then for a given cluster, $c$, and column, $d$, the equation for $P(\theta)$ is just the pdf of the beta function
\begin{equation}
P(\theta_{c,d};\beta_d) = \frac{\Gamma(2\beta_d)}{\Gamma(\beta_d)^2}\theta_{c,d}^{\beta_d-1}(1-\theta_{c,d})^{\beta_d-1}
\end{equation}

The probability of a given cluster c's d-th column is just the joint probability of the $N_c$ independent events (where order matters)
\begin{equation}
P(\{X_{i,d}\}_{i \in b_c};\theta_{d,c}) = \theta_{d,c}^S (1-\theta_{d,c})^R
\end{equation}

The posterior of the beta distribution is also a beta distribution with prior modified by the actual counts
\begin{equation}
\begin{matrix}
P(\{X_{i,d}\}_{i \in b_c};\beta_d) & = & \int_{\theta_{c,d}} P(\theta_{c,d};\beta_d)P(\{X_{i,d}\}_{i \in b_c}|\theta_{d,c})
\\
\\ & = & \int_{\theta_{c,d}} \frac{\Gamma(2\beta_d)}{\Gamma(\beta_d)^2}\theta_{c,d}^{S_{c,d} + \beta_d-1}(1-\theta_{c,d})^{R_{c,d} + \beta_d-1} 
\\
\\ & = & \frac{\Gamma(2\beta_d)}{\Gamma(\beta_d)^2} \frac{\Gamma(S_{c,d} + \beta_d)\Gamma(R_{c,d} + \beta_d)}{\Gamma(S_{c,d} + R_{c,d} + 2 \beta_d)}
\end{matrix}
\end{equation}

So for a given cluster, $c$, over all columns, $d$,
\begin{equation}
\begin{matrix}
P(X_{i \in b_c};\vec \beta) & = & \frac{\Gamma(2\beta_d)^D}{\Gamma(\beta_d)^{2D}} \prod_d \frac{\Gamma(S_{c,d} + \beta_d)\Gamma(R_{c,d} + \beta_d)}{\Gamma(S_{c,d} + R_{c,d} + 2 \beta_d)}
\\
\\ & \propto & \prod_d \frac{\Gamma(S_{c,d} + \beta_d)\Gamma(R_{c,d} + \beta_d)}{\Gamma(N_c + 2 \beta_d)}
\end{matrix}
\end{equation}

The conditional probability that a new vector $z_i$ belongs to this cluster is what is desired.

\begin{equation}
P( z_i=j | X_{k \in b_j/i} ; \vec \beta) = \frac{P(X_{k \in \{b_j \cup i\} } ; \beta_d)}{P(X_{k \in \{b_j / i\}} ; \beta_d)}
\end{equation}

The gamma terms not incremented by the new vector cancel and only the top most factor of those that are incremented remain.  The final result for the conditional distribution (not including P(Z)) is
\begin{equation}
P( z_i=j | X_{k \in b_j/i} ; \vec \beta) = \prod_d \frac{(S_{j,d} + \beta_d)^{x_{i,d}}(R_{j,d} + \beta_d)^{\sim x_{i,d}}}{N_j + 2 \beta_d}
\end{equation}

Since new clusters have $\theta_{d,c}$ generated by a symmetric $\beta_d$, $E[\theta_{d,c}] = \frac{1}{2}$.  This results in
\begin{equation}
P( z_i=new\_cluster | X_{k \in b_j/i} ; \vec \beta) = \frac{1}{2^D} \text{\textcolor{red}{(?)}}
\end{equation}

\section{Derivation of $P(Z)$}

By definition of the CRP (with clustering numbering starting at zero)
\begin{equation}\
x_n = \begin{cases}
  j & \text{with prob} \frac{N_j}{n-1+\alpha} \text{ for $j < \|B\|$} \\
  J & \text{with prob} \frac{\alpha}{n-1+\alpha} \\
\end{cases}
\end{equation}

So
\begin{equation}
P(z_i=j|Z_{-i};\alpha) = 
\begin{cases}
  \frac{N_{j,-i}}{N-1+\alpha} \text{ for $j < \|B\|$} \\
  \frac{\alpha}{N-1+\alpha}  \text{ for $j = \|B\|$},\\
\end{cases}
\end{equation}

Alternatively,
\begin{equation}
P(z_i=j|Z_{-i};\alpha) = \frac{max(N_{j,-i},1) \cdot \alpha^{j=\|B\|}}{N-1+\alpha}
\end{equation}

\section{$P(Z|X)$}

The equation to use for inference via single-site gibbs sampling is (with j indexing starting at 0)

\begin{equation}
P( z_i=j | Z_{-i},X ; \vec \beta, \alpha) = 
  \frac{max(N_{j,-i},1) \cdot \alpha^{j=\|B\|}}{N-1+\alpha} \prod_d \frac{(S_{j,d} + \beta_d)^{x_{i,d}}(R_{j,d} + \beta_d)^{\sim x_{i,d}}}{N_j + 2 \beta_d} 
\end{equation}

\section{Math for $\alpha$'s MH Kernel}

\begin{equation}
P(\alpha;B) = \frac{\Gamma(\alpha)\alpha^{\|B\|}}{\Gamma(N+\alpha)} \prod_{b_j \in B} \Gamma(\|b_j\|)
\end{equation}

Per W's article on the \href{https://en.wikipedia.org/wiki/Chinese_restaurant_process}{Chinese Restaurant Process}, where $N = \sum_{b_j \in B} \|b_j\|$ is the number of vectors
\newline

$0^{th}$ order explanation: Via exchangeability, p$($final state$)$ = p$($any path that arrives at final state$)$.  

To have arrived at $B$, $\|B\|$ clusters must have been created, hence $\alpha^{\|B\|}$.

For each cluster, $b_i$ to have accumulated from 1 to \{2..$\|b_i\|$\}, the numerator in the probability of that choice must have been each of \{1..$\|b_i-1\|$\}, hence $\Gamma(\|b_i\|)$ for each $b \in B$.

The denominator must have been each of $\alpha$ + \{0..N-1\}, hence $\Gamma(N+\alpha)$ in the denominator and $\Gamma(\alpha)$ in the numerator.  Note also that this makes the probability of the first assignment $\frac{\alpha}{\alpha}=1$.

\subsection{Proposal density}

When is an asymmetric proposal density appropriate?

\section{Math for $\beta_d$'s MH Kernel}

Inference on $\beta_d$ uses the same equation as $P(\{X_{i,d}\}_{i \in b_c};\beta_d)$, but with $\{X_{i,d}\}_{i \in b_c}$ fixed (in the form of $S_{c,d}$ and $R_{c,d}$)

\begin{equation}
\begin{matrix}
P(\beta_d;X) & = & \prod_{b_c \in B} P(\beta_d;\{X_{i,d}\}_{i \in b_c}) \\
\\
 & = & \prod_{b_c \in B} \frac{\Gamma(2\beta_d)}{\Gamma(\beta_d)^2} \frac{\Gamma(S_{c,d} + \beta_d)\Gamma(R_{c,d} + \beta_d)}{\Gamma(N_c + 2 \beta_d)}
\end{matrix}
\end{equation}

\subsection{Proposal density}

When is an asymmetric proposal density appropriate?


\section{PDPMB: Derivation for $P(\vec \gamma|\alpha)$ and $P(\vec N|\vec \gamma)$}

To distribute the data across compute nodes, we draw a multinomial distribution, $\vec \gamma$ from a dirichlet distribution with balanced prior $\alpha$.  This $\alpha$ is the equivalent $\alpha$ for a single model's CRP prior.

The probability of a particular $\vec \gamma$ draw from the dirichlet process is
\begin{equation}
P(\vec \gamma|\alpha) = \frac{\Gamma(\sum_j \alpha_j)}{\prod_j \Gamma(\alpha_j)} \prod_j \gamma_j^{\alpha_j-1}
\end{equation}

for balanced prior $\alpha$ over K compute nodes, this is

\begin{equation}
P(\vec \gamma|\alpha) = \frac{\Gamma(K \alpha_j)}{\Gamma(\alpha_j)^K} \prod_j \gamma_j^{\alpha-1}
\end{equation}

$\vec \gamma$ only determines the probability of a data vector being allocated to a particular node.  To determine the actual data distribution, we draw from a multinomial $\#ROWS$ times.  The $i^{th}$ compute node is assigned ${N_i}$ of the rows

\begin{equation}
\vec N \sim Multinomial(\vec \gamma,\#ROWS)
\end{equation}

Note that $\sum_i N_i = \#ROWS$.

The probabilty of a particular $\vec N$ draw is
\begin{equation}
P(\vec N|\vec \gamma) = \frac{(\sum_i N_i)!}{\prod_i (N_i!)} \prod_i \gamma_i^{N_i}
\end{equation}

The math for shuffling clusters among nodes uses a multinomial distribution with a Dirichlet prior: the posterior distribution is also Dirichlet with modified counts for the prior

\begin{equation}
P( Y | \gamma ) = Dir(\gamma') \propto 
\end{equation}

\end{document}

%%Let $B' = B + f(\delta)$, where $\delta \sim Cat(2\|B\|+1,\frac{\vec{1}}{2\|B\|+1})$ and $f(\delta) = -1^{1+(\delta \text{ mod } 2)} \lfloor \delta/2 \rfloor$ allowing subtraction or addition from each only addition to the new cluster index.
